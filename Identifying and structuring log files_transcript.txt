Welcome to this guided project.
Herbal processing and visualizing logs with Elasticstack.
My name is Ahmed, Sweden, and I'll be your instructor
for this project.
I'm excited to teach you some of the most required skills
in the developed job market today.
I had previously worked as an information system software
engineer in the company where we managed to visualize
millions off application logs using the Elasticstack
to retrieve.
And water air is happening in large production environments
in order to prevent application figures beforehand.
Yeah, this shows how log monitoring is such an important
and powerful tool in any I T operations environment.
This project is for people who are interested in the topic
off develops and log monitoring.
I'll teach you how to design implement your first log
processing and visualization pipeline using Elasticstack.
To get the most out of this project, you should be familiar
with application logs, rejects CSB files, no secret databases
and the next terminal.
By the end of this project, you will be able to design your
very first log pipeline system using Elasticstack
Throughout the tasks, I'll be showing you how to process log
files using Elasticstack.
We will explore the three building blocks off Elasticstack.
Throughout the tasks, I will be showing you how to process
log files.
Using Elasticstack, we'll explore the three building blocks
of Elasticstack IP.
You will start by learning how to ingest and process log
foils on Logstash.
You will get the menu with Grok powerful plugging used
to structure logs using regular expressions.
Also, he will be provided with this year's we lock file
in order to help us with the tasks then moving onwards, you
will understand how logs air stored in Elasticsearch and how
you make Query them using the AP ice.
Lastly, you will visualized colorful death sports up.
You understand the log trans visually on kibana.
Let's now have a brief overview of the right platform on the
right side.
Pan, you see my cloud desktop?
I move my cursor right now on the left side.
The big cloud desktop, you see is yours.
All right, this is a hands on project, So please watch what
I'm doing and follow those steps on your cloud desktop You
to pause my instructions at any time.
Please bring the cursor on top off my cloud does stop
and clipped the pause button.
I'm going too fast or too stew can increase or decrease
the speed of the video by clicking the speed button below the
pause button.
This project will help you visualize and understand Web
access logs, which are produced by most Web service.
Processing these dogs is crucial in any I T operations
department operations.
Engineers could derive peak times when websites are visit,
which are mostly which are mostly visited and which are not.
And http success rates on the Web services, the bottlenecks
and low latency.
What I'm showing you now is the final visualization of logs
on kibana.
You will be able Thio learn how to make this and similar
visualization through our task.
So let's dive right in your cloud desktop.
You'll see a file called Web log.
In this five, you'll find all the logs that we need
to process during our project.
Your first task will be to understand this year's We file,
which is where.
Blogger thug.
Let's open the CSE while using sublime just as I am doing
right now, right click, then goto open with and press open
with sublime text.
You're free to maximize your window, and you may even need
to colorize it.
Using your favorites language, for example, I'll use SQL
as our template to just colorize the full five.
Well, let's explain the fight.
This file is a comma delimited file, meaning If you open,
for example, this file on spreadsheets or excel, you'll find
each piece off texts between commas.
They're lined and put into a separate columns.
Let's take an example to make things clear.
For example, the piece of string, the first piece of string
would you have on your screen now is the line of access logs
generated by our generic app, the fierce piece of the text,
which I am highlighting right now.
It should be put in the first column.
The second piece of text, which is which I am highlighting
right now, also should be put into the second column and so
on so forth.
For example, in the fourth column, you'll find the number 200.
After inspecting the file, we found out that the first column
holds the origin I p or host name.
This is the type where the machine is sending its requests.
The second column holds the timestamp of the request, which
is the date and time the request had reached the server.
The third column has the http get request, which is
the beaver, which has the http verb, which is a get request
in our first line and the girl would get requests happened.
Then we have the http 1.1 protocol in the same.
All of these are in the same third column.
Lastly, in the last column we have the http status code which
is in our example 200 which means success.
As you can notice the log file.
It's very structured as it is already comma separated in real
life scenarios.
This is hardly the case.
It is crucial to view and understand the log file
before doing any set up for Elasticstack as by understanding
the data, you will know what to feed in the pipeline.
In order to understand what you would need to visualize
in order to make sure you understood the content so far, try
and adding at the top first off the file ah logline, which
represents the data on this on the screen, feel free to pause
the video and come up with the logline to be added in the log
file, resume the video and I'll show you my solution now
I approached it.
Welcome back.
Now let's compare results as stated the first IP.
The first thing the first column is the IPE.
The second column is the date and time.
Thirdly, we have the http verb with the U. R L and they
should be possible.
Lastly, we have the http request status, which is for all
four. Great job on completing your very first task on the
this project.
In this task, we learned how to read and understand access
log files, which are based in any log logging Web application.
The next task will understand what Logstash is and how
it fits into the six tech and how we could process our log
files using it.